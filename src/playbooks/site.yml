---
# run with:
# cd /opt/meza/ansible
# sudo -u meza-ansible ansible-playbook site.yml

# FIXME #729: Move this into a role
- hosts: localhost
  become: true
  tasks:
  # Can't run set-vars yet, so bootstrap m_home and m_config_vault for now.
  # Meza install path retrieved in same way in set-vars role
    - name: Get Meza install path
      shell: dirname $(dirname $(dirname $(dirname $(realpath $(which meza)))))
      register: ip
      # In --check mode, the shell command will not run, which means there will be no .stdout to pass to set_fact:
      # Since we know THIS shell command has no side-effects, we need to force it to run ALWAYS, even in --check mode.
      # To force a task to run (which could make changes to the system), even when the playbook is called with --check,
      # set check_mode: false.
      check_mode: false
    - set_fact:
        m_home: "{{ ip.stdout }}/conf-meza/users"
        m_config_vault: "{{ ip.stdout }}/conf-meza/vault"
        m_conf_dir: "{{ ip.stdout }}/conf-meza"
    - name: Ensure no password on meza-ansible user on controller
      shell: passwd --delete meza-ansible
      failed_when: false
    - name: Ensure controller has user alt-meza-ansible
      user:
        name: alt-meza-ansible
        move_home: true
        home: "{{ m_home }}/alt-meza-ansible"
        group: wheel
    - name: Ensure user alt-meza-ansible .ssh dir configured
      file:
        path: "{{ m_home }}/alt-meza-ansible/.ssh"
        owner: alt-meza-ansible
        group: wheel
        mode: 0700
        state: directory
    - name: Copy meza-ansible keys to alt-meza-ansible
      copy:
        src: "{{ m_home }}/meza-ansible/.ssh/{{ item.name }}"
        dest: "{{ m_home }}/alt-meza-ansible/.ssh/{{ item.name }}"
        owner: alt-meza-ansible
        group: wheel
        mode: "{{ item.mode }}"
      with_items:
        - name: id_rsa
          mode: "0600"
        - name: id_rsa.pub
          mode: "0644"
    - name: Copy meza-ansible known_hosts to alt-meza-ansible
      copy:
        src: "{{ m_home }}/meza-ansible/.ssh/{{ item.name }}"
        dest: "{{ m_home }}/alt-meza-ansible/.ssh/{{ item.name }}"
        owner: alt-meza-ansible
        group: wheel
        mode: "{{ item.mode }}"
      failed_when: false
      with_items:
        - name: known_hosts
          mode: "0600"
  # Note: without this, if the user decides to encrypt secret.yml with Ansible
  # vault then it changes mode to 0600 and ownership to root:root. This makes it
  # impossible to include_vars later.

    - name: Ensure secret.yml owned by meza-ansible
      file:
        path: "{{ m_conf_dir }}/secret/{{ env }}/secret.yml"
        owner: meza-ansible
        group: wheel
        mode: "0660"
    - name: Ensure {{ m_conf_dir }} owned by meza-ansible
      file:
        path: "{{ m_conf_dir }}"
        owner: meza-ansible
        group: wheel
        mode: "0755"


# Set umask on all servers. Perhaps this should move above localhost steps since
# it may impact them, and role "umask-set" has no config requirements

- hosts: all:!exclude-all:!load_balancers_nonmeza:!load_balancers_nonmeza_external:!load_balancers_nonmeza_internal
  become: true
  roles:
    - umask-set
- hosts: localhost
  become: true
  roles:
    - set-vars
    - init-controller-config
    - role: autodeployer
      when: force_deploy is defined or autodeployer is defined
    - enforce-meza-version
  tags:
    - autodeployer
    - controller
- hosts: localhost
  become: true
  roles:
    - set-vars
    - role: autodeployer
      when: force_deploy is defined or autodeployer is defined

# Ensure proper base setup on all servers in inventory, with the exception of
# servers in "exclude-all" group. At present, the intent of this group is to
# allow servers which serve as sources for database and user-uploaded files,
# but are not managed by this meza install.
- hosts: all:!exclude-all:!load_balancers_nonmeza:!load_balancers_nonmeza_external:!load_balancers_nonmeza_internal
  become: true
  roles:
    - set-vars
    - base
    - base-config-scripts
    # right now remi repo is buried in the apache-php role, but we could do a
    # general best-practice role re-use in our playbook like this
    # - role: geerlingguy.repo-remi
    #   when:
    #     - ansible_facts['distribution'] == "Rocky"
    #     - ansible_facts['distribution_major_version'] == "8"
  tags: base

- hosts: load_balancers,load_balancers_meza_internal,load_balancers_meza_external
  become: true
  tags:
    - load-balancer
    - load_balancers
    - haproxy
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 8001
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      # NOTE: firewalld ports 80 and 443 opened to ALL within haproxy role
    - haproxy
- hosts: app_servers
  become: true
  tags: apache-php
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 8080
      firewall_protocol: tcp
      firewall_servers: "{{ load_balancers_all }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - base-extras
    - imagemagick
    - apache-php
    - role: netdata
      when: m_install_netdata
    - role: firewall_port
      firewall_action: open
      firewall_port: 19999
      firewall_protocol: tcp
      firewall_servers: "{{ load_balancers_all }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"

# Keeping this as a separate play, since at some point it will make sense to
# configure app_servers as gluster clients and have separate gluster servers
# (which can also be app_servers, of course, like everything else in meza)
- hosts: app_servers
  become: true
  tags: gluster
  vars:
    gluster_replicas: "{{ groups['app_servers'] | length }}"
  roles:
    - set-vars

    # Portmap tcp and udp
    #
    # From docs:
    # ref: http://gluster.readthedocs.io/en/latest/Administrator%20Guide/Troubleshooting/?highlight=38465
    #
    # Check your firewall setting to open ports 111 for portmap
    # requests/replies and Gluster NFS server requests/replies. Gluster NFS
    # server operates over the following port numbers: 38465, 38466, and 38467.
    - role: firewall_port
      firewall_action: open
      firewall_port: 111
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1
    - role: firewall_port
      firewall_action: open
      firewall_port: 111
      firewall_protocol: udp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1

    # All gluster servers, Gluster Daemon
    # maybe also: 24009, 24010, 24011
    #
    # From the docs
    # ref: https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Setting%20Up%20Clients/
    #
    # Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster
    # servers. Apart from these ports, you need to open one port for each brick
    # starting from port 49152 (instead of 24009 onwards as with previous
    # releases). The brick ports assignment scheme is now compliant with IANA
    # guidelines. For example: if you have five bricks, you need to have ports
    # 49152 to 49156 open.
    - role: firewall_port
      firewall_action: open
      firewall_port: 24007
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1
    - role: firewall_port
      firewall_action: open
      firewall_port: 24008
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1


    # One port required for each brick (in meza, this equals each server (one
    # brick per server))
    - role: firewall_port
      firewall_action: open
      firewall_port: 49152
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1
    - role: firewall_port
      firewall_action: open
      firewall_port: 49153
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1
    - role: firewall_port
      firewall_action: open
      firewall_port: 49154
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1
    - role: firewall_port
      firewall_action: open
      firewall_port: 49155
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: groups['app_servers']|length|int > 1

      # FIXME #801: Some docs said ports 38465, 38466, 38467 needed for Gluster
    - role: gluster
      when: groups['app_servers']|length|int > 1
- hosts: memcached_servers
  become: true
  tags: memcached
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 11211
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - memcached
- hosts: db_master
  become: true
  tags: database
  roles:
    - set-vars
    - role: firewall_service
      firewall_service: mysql
      # FIXME: try merging these two mysql lines with:
      # firewall_servers: "{{ groups['app_servers'] }} + {{ groups['db_slaves'] }}"
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_service
      firewall_service: mysql
      firewall_servers: "{{ groups['db_slaves'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: database
      # Get the one and only server that should be in the db_master group and set
      # it's IP address as replication master IP. Note that this can be left blank
      # or not included at all if no replication should be performed.
      # FIXME #802: Should replication master be set only if slaves exist?
      mysql_replication_master: "{{ groups['db_master'][0] }}"
      mysql_replication_role: master
- hosts: db_slaves
  become: true
  tags: database
  roles:
    - set-vars
    - role: firewall_service
      firewall_service: mysql
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: ansible_distribution_file_variety == 'RedHat'
    - role: database
      # Get the one and only server that should be in the db_master group and set
      # it's IP address as replication master IP. Note that this can be left blank
      # or not included at all if no replication should be performed.
      mysql_replication_master: "{{ groups['db_master'][0] }}"
      mysql_replication_role: slave
- hosts: backup_servers
  become: true
  tags: backup_servers
  roles:
    - set-vars
    - backup-config
- hosts: elastic_servers
  become: true
  tags: elasticsearch
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 9200
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9300
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9200
      firewall_protocol: tcp
      firewall_servers: "{{ groups['elastic_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9300
      firewall_protocol: tcp
      firewall_servers: "{{ groups['elastic_servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - elasticsearch

# Note: this is app_servers again, but must be after everything else is setup
- hosts: app_servers
  become: true
  tags: mediawiki
  roles:
    - set-vars
    - htdocs
    - role: lua
      when: m_install_lua
    - mediawiki
- hosts: logging_servers
  become: true
  tags: logging
  roles:
    - set-vars
    - role: meza-log
      when: docker_skip_tasks is not defined or not docker_skip_tasks
- hosts: backup_servers
  become: true
  tags:
    - backup-cleanup
  roles:
    - set-vars
    - role: backups-cleanup
      when: backups_cleanup is defined and backups_cleanup.crontime is defined
- hosts: all:!exclude-all:!load_balancers_nonmeza:!load_balancers_nonmeza_external:!load_balancers_nonmeza_internal
  become: true
  tags:
    - cron
    - always
  roles:
    - set-vars
    - role: cron
      when: docker_skip_tasks is not defined or not docker_skip_tasks
    - umask-unset
